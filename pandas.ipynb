{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "homelessness_df = pd.read_csv('homelessness data.csv') # read csv file into a dataframe object\n",
    "\n",
    "\n",
    "# loading inbuilt datasets in python\n",
    "from sklearn import datasets\n",
    "wine_data = pd.DataFrame(datasets.load_wine().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.head()) # print the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.info()) # shows information on each of the columns, such as the data type and number of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.shape) # returns the number of rows and columns in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.describe) #calculates a few summary statistics for each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.values) # returns the data in the dataframe as a 2D NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.columns) # returns the column names of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homelessness_df.index) # returns the index of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homelessness_df.sort_values(\"state\",ascending = False) # sort the dataframe by the values in the \"region\" column in descending order.\n",
    "# if ascending = True, it will sort in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by multiple variables\n",
    "homelessness_df.sort_values([\"individuals\",\"family_members\"],ascending = [False,True]) # sort by \"individuals\" in descending order then \"family_members\" in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsetting rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# susetting sigle columns\n",
    "homelessness_df[\"state\"] # returns all the values in the \"state\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly subsetting multiple columns\n",
    "homelessness_df[[\"state\",\"individuals\"]] # returns the \"state\" and \"individuals\" \n",
    "\n",
    "#To select multiple columns, you need two pairs of square brackets. In this code, the inner and outer square brackets are performing different tasks. The outer square brackets are responsible for subsetting the DataFrame, and the inner square brackets are creating a list of column names to subset. This means you could provide a separate list of column names as a variable and then use that list to perform the same subsetting. Usually, it's easier to do in one line.\n",
    "\n",
    "# or\n",
    "\n",
    "df =[\"state\" , \"individuals\"]\n",
    "homelessness_df[df] # returns the \"state\" and \"individuals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting row with condidition\n",
    "homelessness_df[\"individuals\"] > 10000 # returns boolean values for rows where the \"individuals\" column is greater than 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homelessness_df[homelessness_df[\"individuals\"] > 10000] # returns the rows(text) where the \"individuals\" column is greater than 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting based on text data\n",
    "homelessness_df[homelessness_df[\"state\"] == \"Alabama\"] # returns the rows where the \"state\" column is \"Alabama\"\n",
    "\n",
    "# we can also subset based on dates and times if the column is in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting based on multiple conditions\n",
    "\n",
    "is_state = homelessness_df[\"state\"] == \"Alabama\"\n",
    "is_individuals = homelessness_df[\"individuals\"] == 2570.0\n",
    "homelessness_df[is_state & is_individuals] # returns the rows where the \"state\" column is \"Alabama\" and the \"individuals\" column is 2570\n",
    "\n",
    "# or\n",
    "\n",
    "homelessness_df[ (homelessness_df[\"state\"] == \"Alabama\") & (homelessness_df[\"individuals\"] == 2570.0) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting using .isin()\n",
    "if you want to filter on multiple values of a categorical variable, the easiest way is to use the isin method. This takes in a list of values to filter for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting using .isin()\n",
    "#if you want to filter on multiple values of a categorical variable, the easiest way is to use the isin method. This takes in a list of values to filter for.\n",
    "\n",
    "is_alabama_or_NewYork = homelessness_df[\"state\"].isin([\"Alabama\",\"New York\"])\n",
    "homelessness_df[is_alabama_or_NewYork] # returns the rows where the \"state\" column is \"Alabama\" or \"New York\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding total col as sum of individuals and family_members\n",
    "homelessness_df[\"total\"] = homelessness_df[\"individuals\"] + homelessness_df[\"family_members\"]\n",
    "\n",
    "# Adding p_individuals col as proportion of total that are individuals\n",
    "homelessness_df[\"p_individuals\"] = homelessness_df[\"individuals\"] / homelessness_df[\"total\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('Walmart sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean of weekly_sales\n",
    "print(sales_df[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales_df[\"weekly_sales\"].median())\n",
    "\n",
    "# Print the maximum of the date column\n",
    "print(sales_df[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales_df[\"date\"].min())\n",
    "\n",
    "# similalry we can print mode, std, sum, cumsum, cumprod, quantile etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom IQR(inter quartile range) function to calculate IQR of a column\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales_df[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort sales_1_1 by date\n",
    "sales_df = sales_df.sort_values(\"date\")\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_df[\"cum_weekly_sales\"] = sales_df[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_df[\"cum_max_sales\"] = sales_df[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_df[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate names\n",
    "\n",
    "# Drop duplicate store\n",
    "store_types = sales_df.drop_duplicates(subset=[\"store\"]) #\n",
    "print(store_types)\n",
    "\n",
    "# we'll extract a store with each store from the dataset once. We can do this using the drop_duplicates method. It takes an argument, subset, which is the column we want to find our duplicates based on - in this case, we want all the unique stores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate pairs\n",
    "store_types = sales_df.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "store_types\n",
    "\n",
    "# It'll drop any row where the combination of store and type is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales_df[sales_df[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n",
    "holiday_dates\n",
    "\n",
    "# gives the rows where is_holiday is True and then drops any duplicate dates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting\n",
    "\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "store_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_all = sales_df[\"weekly_sales\"].sum() # Calc total weekly sales\n",
    "sales_A = (sales_df[sales_df[\"type\"]==\"A\"][\"weekly_sales\"]).sum() # calculating sum of weekly sales for store type B\n",
    "sales_B = sales_df[sales_df[\"type\"]==\"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "sales_propn_by_type = [sales_A, sales_B] / sales_all # calculating proportion of sales for each store type\n",
    "print(sales_propn_by_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using groupby to calculate sum of weekly sales for each store type\n",
    "sales_by_type = sales_df.groupby(\"type\")[\"weekly_sales\"].sum() # grouping by type(column in sales_df) and calculating weekly_sales of each type\n",
    "sales_propn_by_type = sales_by_type / sum(sales_by_type)\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type and is_holiday and calculating total weekly sales\n",
    "sales_by_type_is_holiday = sales_df.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum() \n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales_df.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median]) #\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales_df.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot tables\n",
    "Pivot tables are another way of calculating grouped summary statistics\n",
    "the \"values\" argument is the column that you want to summarize, and the index column is the column that you want to group by.\n",
    "By default, pivot_table takes the mean value for each group.\n",
    "If we want a different summary statistic, we can use the aggfunc argument and pass it a function. for example,we can use (aggfunc = np.sum)to get the total sales for each store type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales_df.pivot_table(values=\"weekly_sales\", index=\"type\")\n",
    "print(mean_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales_df.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean, np.median])\n",
    "print(mean_med_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot on two variables\n",
    "\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales_df.pivot_table(values = \"weekly_sales\" , index = \"type\", columns=\"is_holiday\") \n",
    "print(mean_sales_by_type_holiday)\n",
    "\n",
    "#To group by two variables, we can pass a second variable name into the columns argument.\n",
    "#There may be NaNs, or missing values, because there are no any element in our dataset, for example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values(i.e Nan) with 0\n",
    "print(sales_df.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales_df.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0 , margins= True))\n",
    "\n",
    "#if we set the margins argument to True, the last row and last column of the pivot table contain the mean of all the values in the column or row, not including the missing values that were filled in with Os."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and indexing dataframes\n",
    "Index makes subsetting simpler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = pd.read_csv('temperatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the index of temperature to city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "temperatures_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True)) # drop = True will remove the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "print(temperatures_ind.loc[cities]) # subsetting the rows of cities Moscow and Saint Petersburg ; # shows the rows of cities Moscow and Saint Petersburg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\" , \"city\"]) # To take details of multiple cities in a country, we can set the index to be a list of columns, such as [\"country\", \"city\"].\n",
    "\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")] # It's a list of tuples, where each tuple is a pair of country and city. It gives the details of county Brazil with city Rio De Janeiro and country Pakistan with city Lahore.\n",
    "\n",
    "print(temperatures_ind.loc[rows_to_keep]) # subsetting the rows of countries Brazil and Pakistan and cities Rio De Janeiro and Lahore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level=\"city\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing and subsetting with .loc and .iloc\n",
    "\n",
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"]) # Get the rows from Pakistan to Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010\":\"2011\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"Aug 2010\":\"Feb 2011\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-30\")]\n",
    "print(temperatures_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "print(temperatures_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010\":\"2012\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22,1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:,2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures['date'] = pd.to_datetime(temperatures.date) #The data type of date is currently `object`, so Pandas does not know that this column is a date. We can convert it into a `datetime` column using the `pd.to_datetime` method.\n",
    "\n",
    "# Add a year column to temperatures\n",
    "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"Aug 2010\":\"Feb 2012\"])\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(\"avg_temp_c\", index = [\"country\", \"city\"], columns = \"year\")\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\", \"Cairo\"):(\"India\", \"Delhi\"), \"2005\":\"2010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "print(mean_temp_by_year)\n",
    "print()\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "print()\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\") # axis = \"columns\" will take the mean of each row\n",
    "print(mean_temp_by_city)\n",
    "print()\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling missing values\n",
    "In a pandas DataFrame, missing values are indicated with N-a-N, which stands for \"not a number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = pd.read_csv('employees.csv')\n",
    "employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(employees.isna()) # returns boolean values for missing values of whole dataframe\n",
    "                        # True for missing values and False for non-missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(employees.isna().any()) # returns boolean values for missing values in each column of dataframe avocados_2016 : True means NaN values are present in that column and False means no NaN values are present in that column\n",
    "\n",
    "print()\n",
    "\n",
    "print(employees.isna().sum()) # returns the number of missing values in each column of dataframe avocados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of missing values by variable\n",
    "employees.isna().sum().plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values\n",
    "employees_complete = employees.dropna() # removes the rows with missing values\n",
    "# if you want to remove the columns with missing values, you can use the axis argument and set it to 1.\n",
    "\n",
    "print(employees_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns with missing values\n",
    "cols_with_non_missing_numericals = [\"Salary\", \"Bonus %\"]\n",
    "\n",
    "# Create histograms showing the distributions cols_with_missing\n",
    "employees[cols_with_non_missing_numericals].hist()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_filled = employees.fillna(0) # fills the missing values with 0 and returns a new dataframe\n",
    "print(employees_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataframe\n",
    "Two ways of creating dataframes\n",
    "\n",
    "i) from list of dictionaries --> constructed row by row\n",
    "\n",
    "ii) from discitionaries of lists --> constructed column by column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dictionaries - by row \n",
    "\n",
    "list_of_dicts = [\n",
    "                {\"name\": \"Ginger\", \"breed\": \"Dachshund\", \"height_cm\": 22, \"weight_kg\": 10, \"date_of_birth\": \"2019-03-14\"},\n",
    "                 {\"name\": \"Scout\", \"breed\": \"Dalmatian\", \"height_cm\": 59, \"weight_kg\": 25, \"date_of_birth\": \"2019-05-09\"}\n",
    "                 ]\n",
    "# Here, each dictionary represents a row of data. The keys are the column names, and the values are the data points.\n",
    "# for each dictionary, the keys must be the same, and the values must be of the same data type.\n",
    "# The order of the keys in the dictionaries does not matter.\n",
    "# The keys in the dictionaries must match the column names of the DataFrame. If a key is missing, Pandas will fill it in with NaN.\n",
    "# in this dictionary keys(name, breed, height_cm, weight_kg, date_of_birth) are the columns of the dataframe and the corresponding values are the data points.\n",
    "\n",
    "new_dogs = pd.DataFrame(list_of_dicts) # create a dataframe from the list of dictionaries\n",
    "new_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of lists - by column\n",
    "\n",
    "dict_of_lists = {\n",
    "                    \"name\": [\"Ginger\", \"Scout\"],\n",
    "                    \"breed\": [\"Dachshund\", \"Dalmatian\"],\n",
    "                    \"height_cm\": [22, 59],\n",
    "                    \"weight_kg\": [10, 25],\n",
    "                    \"date_of_birth\": [\"2019-03-14\", \"2019-05-09\"]\n",
    "                }   \n",
    "# key = column name\n",
    "# value = list of column values from top to bottom.\n",
    "\n",
    "new_dogs = pd.DataFrame(dict_of_lists) # create a dataframe from the dictionary of lists\n",
    "new_dogs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
